{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e429899b-87e8-4dba-a5fc-f49433511f32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os, random, hashlib, json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Setup Logging\n",
    "log_dir = \"logs\"\n",
    "if not os.path.exists(log_dir): os.makedirs(log_dir)\n",
    "\n",
    "log_path = f\"{log_dir}/run_{datetime.now().strftime('%Y%m%d_%H%M')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s | %(levelname)s | %(message)s',\n",
    "    handlers=[logging.FileHandler(log_path), logging.StreamHandler()]\n",
    ")\n",
    "logging.info(\"Starting ETL process...\")\n",
    "\n",
    "# 2. Fix Seeds\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# 3. Hash CSVs for Reproducibility\n",
    "hashes = {}\n",
    "for file in ['menu_items.csv', 'order_details.csv']:\n",
    "    path = f\"data/{file}\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        hashes[file] = hashlib.sha256(f.read()).hexdigest()\n",
    "    logging.info(f\"Hash for {file}: {hashes[file]}\")\n",
    "\n",
    "with open(\"data_hashes.json\", \"w\") as f:\n",
    "    json.dump(hashes, f)\n",
    "\n",
    "# 4. Capture Environment\n",
    "%pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d12c5bfe-4a92-4bb3-8b3c-b2bfb35dcfb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "def run_etl():\n",
    "    logging.info(\"Starting data loading process...\")\n",
    "    \n",
    "    # 1. Load Data\n",
    "    menu_df = pd.read_csv(\"data/menu_items.csv\")\n",
    "    orders_df = pd.read_csv(\"data/order_details.csv\")\n",
    "\n",
    "    # 2. Clean \n",
    "    menu_df['item_name'] = menu_df['item_name'].str.strip()\n",
    "    menu_df['category'] = menu_df['category'].str.strip()\n",
    "    \n",
    "    # Handle mixed date formats (e.g., 1/1/23)\n",
    "    orders_df['order_date'] = pd.to_datetime(orders_df['order_date'], format='mixed')\n",
    "    \n",
    "    # Robust Time Handling\n",
    "    orders_df['order_time'] = orders_df['order_time'].astype(str).str.strip()\n",
    "    orders_df['order_time'] = orders_df['order_time'].apply(lambda x: x if len(x) > 5 else f\"{x}:00\")\n",
    "    orders_df['order_time'] = pd.to_timedelta(orders_df['order_time'])\n",
    "    \n",
    "    orders_df = orders_df.dropna()\n",
    "\n",
    "    # 3. Join\n",
    "    df = pd.merge(orders_df, menu_df, left_on='item_id', right_on='menu_item_id')\n",
    "\n",
    "    # 4. Tidy Table \n",
    "    tidy_df = df[['order_id', 'order_date', 'order_time', 'item_name', 'category', 'price']].copy()\n",
    "    \n",
    "    # 5. Compute Metrics\n",
    "    top_5 = tidy_df['item_name'].value_counts().nlargest(5)    \n",
    "    rev_by_cat = tidy_df.groupby('category')['price'].sum()    \n",
    "    tidy_df['hour'] = tidy_df['order_time'].dt.components['hours']\n",
    "    busiest_hour = tidy_df['hour'].value_counts().idxmax()\n",
    "\n",
    "    logging.info(f\"Metrics computed. Busiest hour identified: {busiest_hour}:00\")\n",
    "\n",
    "    # 6. Assert Tests \n",
    "    assert not tidy_df.empty, \"Error: The resulting DataFrame is empty.\"\n",
    "    assert 'price' in tidy_df.columns, \"Error: Price column is missing after join.\"\n",
    "    \n",
    "    # 7. Save Results \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "    output_file = f\"data/metrics_{timestamp}.csv\"\n",
    "    rev_by_cat.to_csv(output_file)\n",
    "    \n",
    "    logging.info(f\"ETL successful. Metrics saved to: {output_file}\")\n",
    "    return tidy_df\n",
    "\n",
    "final_data = run_etl()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8966637289546391,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "lab_2_4_repro_logging",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
